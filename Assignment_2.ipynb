{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8yOH7N0SURyJG+zaP0ON+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajnCoder/assignment2/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Q1 and Q2, and one other question.\n"
      ],
      "metadata": {
        "id": "GurEyvptDiQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.**\n",
        "1. According to the abstract, this paper is about data tidying. It highlights how tidy datasets are easier to work with, manipulate, model and visualize. They enable the development of more efficient data analysis tools, as demonstrated in a case study.\n",
        "2. The \"tidy data standard\" intended to accomplish a standardized way of organizing data within a dataset. They want to facilitate initial exploration and simply the development of data analysis tools that work well together.\n",
        "3. What I understood from the first sentence about families was that tidy datasets follow a standard structure and messy datasets can vary widely in how they are disorganized or structured. What I understood from the second sentence was that when looking at a specific dataset, it's easy to determine the observations and variables, but when looking at it in a broader context and not a specific dataset, it's harder to define the observations and variables.\n",
        "4. Wickham defines values as either numbers, if it's quantitative, or strings, if it's qualitative. He defines variables as containing all values that measure the same underlying attribute across units. And he defines observations as containing all values measured on the same unit across attributes.\n",
        "5. In this section, \"tidy data\" is defined as a standard way of mapping the meaning of a dataset to its structure. In a tidy dataset, each variable forms a column, each observation forms a row, and each observational unit forms a table.\n",
        "6. The 5 most common problems with messy datasets are: 1. Column headers are values, not variable names. 2. Multiple variables are stored in one column. 3. Variables are stored in both rows and columns. 4. Multiple types of observational units are stored in the same table. 5. A single observational unit is stored in multiple tables. The data in table 4 is messy, because the column headers are values not variable names. \"Melting\" a dataset refers to the act of turning columns into rows.\n",
        "7. Table 11 is a messy dataset, but Table 12 is tidy and \"molten\", because in Table 11 variables are stored in both rows and columns. While Table 12a is the same data but melted down (molten), and Table 12b is the same data but melted and after the cast operation of rotating the element variable back out into the columns.\n",
        "8. the chicken-and-egg problem with focusing on tidy data is that if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data, making it get stuck in a local maxima. Wickham hopes to use methodologies from various fields to improve our understanding of the cognitive side of data analysis, and to further improve our ability to design appropriate tools in the future.  "
      ],
      "metadata": {
        "id": "HXAGX9miDyUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.**\n",
        "1. Below I imported the necessary libraries and loaded the file. I went through the data and removed commas, converting it to a numeric type, and identifying missing values."
      ],
      "metadata": {
        "id": "XEgQ3p5sZJe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "utrHU5slDg_3",
        "outputId": "4fd96533-aa14-4de0-c2c4-b7299a06aa87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150      1481\n",
            "100      1207\n",
            "200      1059\n",
            "125       889\n",
            "75        873\n",
            "         ... \n",
            "840         1\n",
            "306         1\n",
            "2,695       1\n",
            "2,520       1\n",
            "291         1\n",
            "Name: Price, Length: 511, dtype: int64 \n",
            "\n",
            "Before coercion: \n",
            " count     30478\n",
            "unique      511\n",
            "top         150\n",
            "freq       1481\n",
            "Name: Price, dtype: object \n",
            "\n",
            "After coercion: \n",
            " count    30478.000000\n",
            "mean       163.589737\n",
            "std        197.785454\n",
            "min         10.000000\n",
            "25%         80.000000\n",
            "50%        125.000000\n",
            "75%        195.000000\n",
            "max      10000.000000\n",
            "Name: Price, dtype: float64 \n",
            "\n",
            "Total Missings: \n",
            " 0 \n",
            "\n",
            "Before coercion: \n",
            " count    30478.000000\n",
            "mean       163.589737\n",
            "std        197.785454\n",
            "min         10.000000\n",
            "25%         80.000000\n",
            "50%        125.000000\n",
            "75%        195.000000\n",
            "max      10000.000000\n",
            "Name: Price, dtype: float64 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Host Id                         int64\n",
              "Host Since                     object\n",
              "Name                           object\n",
              "Neighbourhood                  object\n",
              "Property Type                  object\n",
              "Review Scores Rating (bin)    float64\n",
              "Room Type                      object\n",
              "Zipcode                       float64\n",
              "Beds                          float64\n",
              "Number of Records               int64\n",
              "Number Of Reviews               int64\n",
              "Price                           int64\n",
              "Review Scores Rating          float64\n",
              "Price_nan                        bool\n",
              "dtype: object"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        145\n",
            "1         37\n",
            "2         28\n",
            "3        199\n",
            "4        549\n",
            "        ... \n",
            "30473    300\n",
            "30474    125\n",
            "30475     80\n",
            "30476     35\n",
            "30477     80\n",
            "Name: Price, Length: 30478, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('./data/airbnb_hw.csv',low_memory=False)\n",
        "\n",
        "df.head()\n",
        "df['Price'].unique()\n",
        "print(df['Price'].value_counts(), '\\n')\n",
        "\n",
        "var = 'Price'\n",
        "print('Before coercion: \\n', df[var].describe(),'\\n')\n",
        "\n",
        "df[var] = df[var].astype('str')\n",
        "df[var] = df[var].str.replace(',', '')\n",
        "df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "df[var+'_nan'] = df[var].isnull()\n",
        "\n",
        "# After coercion:\n",
        "print('After coercion: \\n', df[var].describe(),'\\n')\n",
        "print('Total Missings: \\n', sum(df[var+'_nan']),'\\n')\n",
        "\n",
        "print('Before coercion: \\n', df[var].describe(),'\\n')\n",
        "df[var] = df[var].apply(np.int64)\n",
        "display(df.dtypes)\n",
        "\n",
        "print(df[var])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. After loading the necesarry file, I grouped similar Types together and renamed them to simplify the categorization to just Unprovoke, Provoked, and Unverified."
      ],
      "metadata": {
        "id": "FhBE50erunZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('./data/sharks.csv', low_memory=False)\n",
        "\n",
        "#print(df)\n",
        "\n",
        "var = 'Type'\n",
        "#print(df[var].unique, '\\n')\n",
        "print(df[var].value_counts(), '\\n')\n",
        "\n",
        "#df[var] = df[var].replace('nan', np.nan, inplace=True)\n",
        "#df[var] = df[var].replace('Invalid', 'Unprovoked', inplace=True)\n",
        "\n",
        "df[var] = df[var].replace('Boat', 'Provoked')\n",
        "df[var] = df[var].replace('Boatomg', 'Provoked')\n",
        "df[var] = df[var].replace('Boating', 'Provoked')\n",
        "df[var] = df[var].replace('Watercraft', 'Provoked')\n",
        "\n",
        "df[var] = df[var].replace('Unconfirmed', 'Unverified')\n",
        "df[var] = df[var].replace('Sea Disaster', 'Unprovoked')\n",
        "df[var] = df[var].replace('Questionable', 'Unverified')\n",
        "df[var] = df[var].replace('Invalid', 'Unverified')\n",
        "df[var] = df[var].replace('Under investigation', 'Unverified')\n",
        "\n",
        "\n",
        "print(df[var].unique(), '\\n')\n",
        "print(df[var].value_counts(), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUmuVkHtuo27",
        "outputId": "5b50e3cc-11f2-41fa-823b-d8eae75e85ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unprovoked             4716\n",
            "Provoked                593\n",
            "Invalid                 552\n",
            "Sea Disaster            239\n",
            "Watercraft              142\n",
            "Boat                    109\n",
            "Boating                  92\n",
            "Questionable             10\n",
            "Unconfirmed               1\n",
            "Unverified                1\n",
            "Under investigation       1\n",
            "Boatomg                   1\n",
            "Name: Type, dtype: int64 \n",
            "\n",
            "['Unprovoked' 'Provoked' 'Unverified' nan] \n",
            "\n",
            "Unprovoked    4955\n",
            "Provoked       937\n",
            "Unverified     565\n",
            "Name: Type, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Here, I simply loaded a file using a URL, replaced any occurences of the value 9 with nan, because it was a missing value, and then printed out the modified column."
      ],
      "metadata": {
        "id": "WinDtFd6ALuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "#df.head()\n",
        "df = pd.read_csv(url, low_memory=False)\n",
        "\n",
        "var = 'WhetherDefendantWasReleasedPretrial'\n",
        "\n",
        "df[var] = df[var].replace(9,np.nan)\n",
        "print(df[var].value_counts())\n",
        "print(df[var])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zVNoMK8jOKA",
        "outputId": "acc14256-3223-4e77-a3e3-43a0f4d230dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Lastly, after loading my file and creating the variables, I created a crosstabulation with the two columns. Thereby, performing conditional updates on one column based on values in another column, and then displaying the updated tables at the end."
      ],
      "metadata": {
        "id": "9JtS7D5XWqRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "\n",
        "df = pd.read_csv(url, low_memory=False)\n",
        "\n",
        "var = 'ImposedSentenceAllChargeInContactEvent'\n",
        "var1 = 'SentenceTypeAllChargesAtConvictionInContactEvent'\n",
        "\n",
        "print(pd.crosstab(df[var], df[var1]))\n",
        "\n",
        "df.loc[df[var1] == 4, var] = '0'\n",
        "df.loc[df[var1] == 9, var] = np.nan\n",
        "\n",
        "pd.crosstab(df[var], df[var1])\n",
        "print(df[var].value_counts(), '\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "cqpmj8dzAU79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5**\n",
        "\n",
        "1. To my knowledge, the most recent US Census used a self-identification method to gather data on race. Respondents were asked to choose one or more race categories that best described themselves.\n",
        "2. Gathering this type of data is important so we as a society can analyze changes in racial demographics over time and address issues related to equality and representation in areas such as politics, businesses, etc. Which is why it's important that the data quality is accurate, so we don't misallocate resources, underrepresent communities, or pass ineffective policy decisions.\n",
        "3. I think what was done well was that they tried their best to be inclusive and even allowed for write-in responses. However, I think it might be missing the hard-to-reach or undercounted communities. I think future large scale surveys can invest in targeted outreach campaigns to reach those communities to best reflect the diversity of the population.\n",
        "4. The Census gathered data on sex and gender by using a binary male/female question. Then following up by asking what gender they identify with. I think overall the Census did a good job on gathering data for sex and gender.\n",
        "5. My biggest concern when it comes to cleaning data is ensuring privacy, fairness, and represention. Some challenges that I can think will arise with missing data are biased analyses and decisions. I think some good practices we could adopt are prioritizing transperancy, make efforts to improve the data of underrepresented communities and if possible implementing bias-aware data cleaning algorithms to detect biases.\n",
        "6. I think my biggest concern would be that the algorithm would follow the pattern too much and end up with inaccurate data based on stereotypes and assumptions.\n"
      ],
      "metadata": {
        "id": "yYM2DclcF-FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wll3FfBmjHFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-0B8laVHjG_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "*italicized text*"
      ],
      "metadata": {
        "id": "5Q3nRmrPjHmK"
      }
    }
  ]
}